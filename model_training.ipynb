{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning Model Training for Black Spot Alert System\n",
        "\n",
        "This notebook:\n",
        "1. Loads the processed dataset\n",
        "2. Performs feature engineering\n",
        "3. Creates risk labels using quartiles (Low, Medium, High, Very High)\n",
        "4. Splits data into train/test sets\n",
        "5. Trains a Random Forest Classifier\n",
        "6. Evaluates the model (Accuracy, Precision, Recall, F1, Confusion Matrix)\n",
        "7. Saves the trained model as .pkl file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for plots\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Processed Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the processed dataset\n",
        "df = pd.read_csv(\"processed_blackspot_dataset.csv\")\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Feature Engineering\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_features(df):\n",
        "    \"\"\"Create additional features for ML model.\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # Create composite risk indicators\n",
        "    df['total_incidents'] = (\n",
        "        df['accident_count'].fillna(0) + \n",
        "        df['traffic_incident_count'].fillna(0)\n",
        "    )\n",
        "    \n",
        "    # Create risk intensity score (0-10 scale)\n",
        "    # Normalize accident count to 0-10 scale\n",
        "    if df['accident_count'].max() > 0:\n",
        "        df['recent_incidents_score'] = (\n",
        "            (df['accident_count'] / df['accident_count'].max()) * 10\n",
        "        ).clip(0, 10)\n",
        "    else:\n",
        "        df['recent_incidents_score'] = 0\n",
        "    \n",
        "    # Create urban indicator (if tourist density is high, likely urban)\n",
        "    df['is_urban'] = (df['tourist_density'].fillna(0) > df['tourist_density'].median()).astype(int)\n",
        "    \n",
        "    # Create crowding level based on tourist density\n",
        "    tourist_quantiles = df['tourist_density'].fillna(0).quantile([0.33, 0.67])\n",
        "    df['crowding_level_encoded'] = pd.cut(\n",
        "        df['tourist_density'].fillna(0),\n",
        "        bins=[-np.inf, tourist_quantiles[0.33], tourist_quantiles[0.67], np.inf],\n",
        "        labels=[0, 1, 2]  # Low, Medium, High\n",
        "    ).astype(int)\n",
        "    \n",
        "    # Create transport exposure risk (based on traffic volume and accidents)\n",
        "    df['transport_exposure'] = (\n",
        "        df['avg_traffic_volume'].fillna(0) * df['accident_count'].fillna(0)\n",
        "    ) / 1000  # Normalize\n",
        "    \n",
        "    # Create weather risk indicator\n",
        "    weather_risk_map = {\n",
        "        'Storm': 3, 'Fog': 2, 'Snow': 2, 'Rain': 1,\n",
        "        'Clear': 0, 'Unknown': 1\n",
        "    }\n",
        "    df['weather_risk'] = df['dominant_weather'].map(\n",
        "        lambda x: weather_risk_map.get(x, 1)\n",
        "    ).fillna(1)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Create features\n",
        "df = create_features(df)\n",
        "print(\"Features created successfully!\")\n",
        "print(f\"\\nNew feature columns: {[col for col in df.columns if col not in ['City', 'State']]}\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Risk Labels (Target Variable)\n",
        "\n",
        "We'll use quartiles of total incidents to create risk categories:\n",
        "- Q1 (0-25%) → Low\n",
        "- Q2 (25-50%) → Medium  \n",
        "- Q3 (50-75%) → High\n",
        "- Q4 (75-100%) → Very High\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_risk_labels(df):\n",
        "    \"\"\"Create risk labels based on quartiles of total incidents.\"\"\"\n",
        "    # Use total_incidents as the basis for risk classification\n",
        "    incident_counts = df['total_incidents'].fillna(0)\n",
        "    \n",
        "    # Calculate quartiles\n",
        "    q1 = incident_counts.quantile(0.25)\n",
        "    q2 = incident_counts.quantile(0.50)\n",
        "    q3 = incident_counts.quantile(0.75)\n",
        "    \n",
        "    print(f\"Risk Label Thresholds:\")\n",
        "    print(f\"  Q1 (Low): 0 - {q1:.1f}\")\n",
        "    print(f\"  Q2 (Medium): {q1:.1f} - {q2:.1f}\")\n",
        "    print(f\"  Q3 (High): {q2:.1f} - {q3:.1f}\")\n",
        "    print(f\"  Q4 (Very High): {q3:.1f} - {incident_counts.max():.1f}\")\n",
        "    \n",
        "    # Create labels\n",
        "    def assign_risk_level(count):\n",
        "        if count <= q1:\n",
        "            return \"Low\"\n",
        "        elif count <= q2:\n",
        "            return \"Medium\"\n",
        "        elif count <= q3:\n",
        "            return \"High\"\n",
        "        else:\n",
        "            return \"Very High\"\n",
        "    \n",
        "    df['risk_level'] = incident_counts.apply(assign_risk_level)\n",
        "    \n",
        "    # Display distribution\n",
        "    print(f\"\\nRisk Level Distribution:\")\n",
        "    print(df['risk_level'].value_counts().sort_index())\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Create risk labels\n",
        "df = create_risk_labels(df)\n",
        "df[['City', 'State', 'total_incidents', 'risk_level']].head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Features for Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select features for training\n",
        "# Exclude City, State (identifiers) and risk_level (target)\n",
        "feature_columns = [\n",
        "    'accident_count', 'avg_severity', 'max_severity',\n",
        "    'night_accidents', 'weekend_accidents',\n",
        "    'night_accident_rate', 'weekend_accident_rate', 'precipitation_rate',\n",
        "    'avg_temperature', 'avg_visibility', 'avg_wind_speed',\n",
        "    'tourist_visit_count', 'tourist_density', 'avg_stay_nights',\n",
        "    'avg_spending', 'avg_satisfaction',\n",
        "    'avg_traffic_severity', 'max_traffic_severity', 'traffic_incident_count',\n",
        "    'avg_traffic_volume', 'avg_speed',\n",
        "    'total_incidents', 'recent_incidents_score', 'is_urban',\n",
        "    'crowding_level_encoded', 'transport_exposure', 'weather_risk'\n",
        "]\n",
        "\n",
        "# Remove any columns that don't exist\n",
        "available_features = [col for col in feature_columns if col in df.columns]\n",
        "print(f\"Selected {len(available_features)} features for training\")\n",
        "print(f\"Features: {available_features}\")\n",
        "\n",
        "# Prepare X and y\n",
        "X = df[available_features].copy()\n",
        "y = df['risk_level'].copy()\n",
        "\n",
        "# Handle any remaining NaN values\n",
        "X = X.fillna(X.median())\n",
        "\n",
        "print(f\"\\nX shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"\\nTarget distribution:\\n{y.value_counts().sort_index()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split into train and test sets (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"\\nTraining set target distribution:\")\n",
        "print(y_train.value_counts().sort_index())\n",
        "print(f\"\\nTest set target distribution:\")\n",
        "print(y_test.value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Train Random Forest Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train Random Forest Classifier\n",
        "print(\"Training Random Forest Classifier...\")\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=10,\n",
        "    min_samples_split=5,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(\"✅ Model training complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "y_train_pred = rf_model.predict(X_train)\n",
        "y_test_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate metrics\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\\nTraining Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "\n",
        "# Precision, Recall, F1 (macro average)\n",
        "precision = precision_score(y_test, y_test_pred, average='macro')\n",
        "recall = recall_score(y_test, y_test_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "print(f\"\\nPrecision (macro): {precision:.4f}\")\n",
        "print(f\"Recall (macro): {recall:.4f}\")\n",
        "print(f\"F1-Score (macro): {f1:.4f}\")\n",
        "\n",
        "# Per-class metrics\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PER-CLASS METRICS\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_test_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "class_names = sorted(y_test.unique())\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(\n",
        "    cm, annot=True, fmt='d', cmap='Blues',\n",
        "    xticklabels=class_names,\n",
        "    yticklabels=class_names\n",
        ")\n",
        "plt.title('Confusion Matrix - Random Forest Classifier')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(pd.DataFrame(cm, index=class_names, columns=class_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': available_features,\n",
        "    'importance': rf_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 15 Most Important Features:\")\n",
        "print(feature_importance.head(15))\n",
        "\n",
        "# Visualize feature importance\n",
        "plt.figure(figsize=(10, 8))\n",
        "top_features = feature_importance.head(15)\n",
        "sns.barplot(data=top_features, y='feature', x='importance', palette='viridis')\n",
        "plt.title('Top 15 Feature Importance')\n",
        "plt.xlabel('Importance')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Trained Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create label encoder for risk levels\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(['Low', 'Medium', 'High', 'Very High'])\n",
        "\n",
        "# Save model and metadata\n",
        "model_package = {\n",
        "    \"model\": rf_model,\n",
        "    \"label_encoder\": label_encoder,\n",
        "    \"feature_columns\": available_features,\n",
        "    \"scaler\": None,  # Not using scaler for tree-based models\n",
        "    \"risk_thresholds\": {\n",
        "        \"q1\": df['total_incidents'].quantile(0.25),\n",
        "        \"q2\": df['total_incidents'].quantile(0.50),\n",
        "        \"q3\": df['total_incidents'].quantile(0.75)\n",
        "    }\n",
        "}\n",
        "\n",
        "model_path = \"trained_blackspot_model.pkl\"\n",
        "joblib.dump(model_package, model_path)\n",
        "print(f\"✅ Model saved to: {model_path}\")\n",
        "print(f\"\\nModel package contains:\")\n",
        "print(f\"  - Trained Random Forest model\")\n",
        "print(f\"  - Label encoder\")\n",
        "print(f\"  - Feature columns: {len(available_features)} features\")\n",
        "print(f\"  - Risk thresholds\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test Model Loading\n",
        "\n",
        "Verify that the saved model can be loaded correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.12.10' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/vunda/AppData/Local/Microsoft/WindowsApps/python3.12.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "# Test loading the model\n",
        "loaded_package = joblib.load(model_path)\n",
        "loaded_model = loaded_package[\"model\"]\n",
        "loaded_features = loaded_package[\"feature_columns\"]\n",
        "\n",
        "print(\"✅ Model loaded successfully!\")\n",
        "print(f\"Loaded model type: {type(loaded_model)}\")\n",
        "print(f\"Number of features: {len(loaded_features)}\")\n",
        "\n",
        "# Test prediction with a sample\n",
        "sample_idx = 0\n",
        "sample_features = X_test.iloc[sample_idx:sample_idx+1]\n",
        "prediction = loaded_model.predict(sample_features)[0]\n",
        "actual = y_test.iloc[sample_idx]\n",
        "\n",
        "print(f\"\\nSample Prediction Test:\")\n",
        "print(f\"  Actual: {actual}\")\n",
        "print(f\"  Predicted: {prediction}\")\n",
        "print(f\"  Correct: {actual == prediction}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
