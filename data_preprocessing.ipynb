{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preprocessing Pipeline for Black Spot Alert System\n",
        "\n",
        "This notebook:\n",
        "1. Loads accident, tourism, and traffic datasets\n",
        "2. Cleans and standardizes location names\n",
        "3. Merges datasets by city/state\n",
        "4. Creates aggregated features\n",
        "5. Handles missing values\n",
        "6. Exports ML-ready dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m accidents_df, tourism_df, traffic_df\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m accidents_df, tourism_df, traffic_df = \u001b[43mload_datasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mload_datasets\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading datasets...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Load accidents dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m accidents_df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m\"\u001b[39m\u001b[33mus_accidents_10k_sample.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAccidents dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccidents_df.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load tourism dataset\u001b[39;00m\n",
            "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "def load_datasets():\n",
        "    \"\"\"Load all three datasets.\"\"\"\n",
        "    print(\"Loading datasets...\")\n",
        "    \n",
        "    # Load accidents dataset\n",
        "    accidents_df = pd.read_csv(\"us_accidents_10k_sample.csv\")\n",
        "    print(f\"Accidents dataset: {accidents_df.shape}\")\n",
        "    \n",
        "    # Load tourism dataset\n",
        "    tourism_df = pd.read_csv(\"usa_tourism_dataset_10000.csv\")\n",
        "    print(f\"Tourism dataset: {tourism_df.shape}\")\n",
        "    \n",
        "    # Load traffic dataset\n",
        "    traffic_df = pd.read_csv(\"synthetic_heavy_traffic_usa_10k.csv\")\n",
        "    print(f\"Traffic dataset: {traffic_df.shape}\")\n",
        "    \n",
        "    return accidents_df, tourism_df, traffic_df\n",
        "\n",
        "# Load the datasets\n",
        "accidents_df, tourism_df, traffic_df = load_datasets()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview each dataset\n",
        "print(\"\\n=== Accidents Dataset ===\")\n",
        "print(accidents_df.head())\n",
        "print(\"\\n=== Tourism Dataset ===\")\n",
        "print(tourism_df.head())\n",
        "print(\"\\n=== Traffic Dataset ===\")\n",
        "print(traffic_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Helper Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_location_names(df, city_col, state_col):\n",
        "    \"\"\"Standardize city and state names.\"\"\"\n",
        "    if city_col in df.columns:\n",
        "        df[city_col] = df[city_col].str.strip().str.title()\n",
        "    if state_col in df.columns:\n",
        "        df[state_col] = df[state_col].str.strip().str.upper()\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Process Accidents Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_accidents(accidents_df):\n",
        "    \"\"\"Process accidents dataset to create city-level features.\"\"\"\n",
        "    print(\"\\nProcessing accidents data...\")\n",
        "    \n",
        "    # Clean location names\n",
        "    accidents_df = clean_location_names(accidents_df, \"City\", \"State\")\n",
        "    \n",
        "    # Convert Start_Time to datetime\n",
        "    accidents_df[\"Start_Time\"] = pd.to_datetime(accidents_df[\"Start_Time\"], errors=\"coerce\")\n",
        "    \n",
        "    # Extract time features\n",
        "    accidents_df[\"hour\"] = accidents_df[\"Start_Time\"].dt.hour\n",
        "    accidents_df[\"month\"] = accidents_df[\"Start_Time\"].dt.month\n",
        "    accidents_df[\"year\"] = accidents_df[\"Start_Time\"].dt.year\n",
        "    accidents_df[\"is_night\"] = (accidents_df[\"hour\"] >= 20) | (accidents_df[\"hour\"] <= 6)\n",
        "    accidents_df[\"is_weekend\"] = accidents_df[\"Start_Time\"].dt.dayofweek >= 5\n",
        "    \n",
        "    # Aggregate by city and state\n",
        "    city_accidents = accidents_df.groupby([\"City\", \"State\"]).agg({\n",
        "        \"ID\": \"count\",  # Total accident count\n",
        "        \"Severity\": [\"mean\", \"max\"],  # Average and max severity\n",
        "        \"is_night\": \"sum\",  # Night accidents count\n",
        "        \"is_weekend\": \"sum\",  # Weekend accidents count\n",
        "        \"Temperature(F)\": \"mean\",  # Average temperature\n",
        "        \"Visibility(mi)\": \"mean\",  # Average visibility\n",
        "        \"Wind_Speed(mph)\": \"mean\",  # Average wind speed\n",
        "        \"Precipitation(in)\": lambda x: (x > 0).sum(),  # Count of accidents with precipitation\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Flatten column names\n",
        "    city_accidents.columns = [\n",
        "        \"City\", \"State\",\n",
        "        \"accident_count\",\n",
        "        \"avg_severity\",\n",
        "        \"max_severity\",\n",
        "        \"night_accidents\",\n",
        "        \"weekend_accidents\",\n",
        "        \"avg_temperature\",\n",
        "        \"avg_visibility\",\n",
        "        \"avg_wind_speed\",\n",
        "        \"precipitation_accidents\"\n",
        "    ]\n",
        "    \n",
        "    # Calculate proportions\n",
        "    city_accidents[\"night_accident_rate\"] = (\n",
        "        city_accidents[\"night_accidents\"] / city_accidents[\"accident_count\"]\n",
        "    ).fillna(0)\n",
        "    city_accidents[\"weekend_accident_rate\"] = (\n",
        "        city_accidents[\"weekend_accidents\"] / city_accidents[\"accident_count\"]\n",
        "    ).fillna(0)\n",
        "    city_accidents[\"precipitation_rate\"] = (\n",
        "        city_accidents[\"precipitation_accidents\"] / city_accidents[\"accident_count\"]\n",
        "    ).fillna(0)\n",
        "    \n",
        "    print(f\"Processed accidents: {city_accidents.shape[0]} unique cities\")\n",
        "    return city_accidents\n",
        "\n",
        "# Process accidents\n",
        "city_accidents = process_accidents(accidents_df)\n",
        "print(\"\\nAccidents aggregation preview:\")\n",
        "print(city_accidents.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Process Tourism Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_tourism(tourism_df):\n",
        "    \"\"\"Process tourism dataset to create city-level features.\"\"\"\n",
        "    print(\"\\nProcessing tourism data...\")\n",
        "    \n",
        "    # Clean location names\n",
        "    tourism_df = clean_location_names(tourism_df, \"city\", \"state\")\n",
        "    \n",
        "    # Convert visit_date to datetime\n",
        "    tourism_df[\"visit_date\"] = pd.to_datetime(tourism_df[\"visit_date\"], errors=\"coerce\")\n",
        "    \n",
        "    # Aggregate by city and state\n",
        "    city_tourism = tourism_df.groupby([\"city\", \"state\"]).agg({\n",
        "        \"record_id\": \"count\",  # Total tourist visits\n",
        "        \"stay_nights\": [\"mean\", \"sum\"],  # Average and total stay nights\n",
        "        \"avg_spend_usd_per_person\": \"mean\",  # Average spending\n",
        "        \"satisfaction_rating_1_5\": \"mean\",  # Average satisfaction\n",
        "        \"party_size\": \"mean\",  # Average party size\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Flatten column names\n",
        "    city_tourism.columns = [\n",
        "        \"City\", \"State\",\n",
        "        \"tourist_visit_count\",\n",
        "        \"avg_stay_nights\",\n",
        "        \"total_stay_nights\",\n",
        "        \"avg_spending\",\n",
        "        \"avg_satisfaction\",\n",
        "        \"avg_party_size\"\n",
        "    ]\n",
        "    \n",
        "    # Calculate tourist density (visits per city)\n",
        "    city_tourism[\"tourist_density\"] = city_tourism[\"tourist_visit_count\"]\n",
        "    \n",
        "    print(f\"Processed tourism: {city_tourism.shape[0]} unique cities\")\n",
        "    return city_tourism\n",
        "\n",
        "# Process tourism\n",
        "city_tourism = process_tourism(tourism_df)\n",
        "print(\"\\nTourism aggregation preview:\")\n",
        "print(city_tourism.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Process Traffic Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_traffic(traffic_df):\n",
        "    \"\"\"Process traffic dataset to create city-level features.\"\"\"\n",
        "    print(\"\\nProcessing traffic data...\")\n",
        "    \n",
        "    # Clean location names\n",
        "    traffic_df = clean_location_names(traffic_df, \"City\", \"State\")\n",
        "    \n",
        "    # Convert Timestamp to datetime\n",
        "    traffic_df[\"Timestamp\"] = pd.to_datetime(traffic_df[\"Timestamp\"], errors=\"coerce\")\n",
        "    \n",
        "    # Aggregate by city and state\n",
        "    city_traffic = traffic_df.groupby([\"City\", \"State\"]).agg({\n",
        "        \"Severity\": [\"mean\", \"max\", \"count\"],  # Traffic severity metrics\n",
        "        \"Traffic_Volume\": \"mean\",  # Average traffic volume\n",
        "        \"Avg_Speed\": \"mean\",  # Average speed\n",
        "        \"Weather_Impact\": lambda x: x.value_counts().index[0] if len(x) > 0 else \"Unknown\",  # Most common weather\n",
        "    }).reset_index()\n",
        "    \n",
        "    # Flatten column names\n",
        "    city_traffic.columns = [\n",
        "        \"City\", \"State\",\n",
        "        \"avg_traffic_severity\",\n",
        "        \"max_traffic_severity\",\n",
        "        \"traffic_incident_count\",\n",
        "        \"avg_traffic_volume\",\n",
        "        \"avg_speed\",\n",
        "        \"dominant_weather\"\n",
        "    ]\n",
        "    \n",
        "    print(f\"Processed traffic: {city_traffic.shape[0]} unique cities\")\n",
        "    return city_traffic\n",
        "\n",
        "# Process traffic\n",
        "city_traffic = process_traffic(traffic_df)\n",
        "print(\"\\nTraffic aggregation preview:\")\n",
        "print(city_traffic.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Merge All Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def merge_datasets(city_accidents, city_tourism, city_traffic):\n",
        "    \"\"\"Merge all processed datasets on City and State.\"\"\"\n",
        "    print(\"\\nMerging datasets...\")\n",
        "    \n",
        "    # Start with accidents as base (most comprehensive)\n",
        "    merged_df = city_accidents.copy()\n",
        "    \n",
        "    # Merge tourism data\n",
        "    merged_df = merged_df.merge(\n",
        "        city_tourism,\n",
        "        on=[\"City\", \"State\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        "    \n",
        "    # Merge traffic data\n",
        "    merged_df = merged_df.merge(\n",
        "        city_traffic,\n",
        "        on=[\"City\", \"State\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        "    \n",
        "    print(f\"Merged dataset shape: {merged_df.shape}\")\n",
        "    print(f\"Unique cities: {merged_df[['City', 'State']].drop_duplicates().shape[0]}\")\n",
        "    \n",
        "    return merged_df\n",
        "\n",
        "# Merge datasets\n",
        "merged_df = merge_datasets(city_accidents, city_tourism, city_traffic)\n",
        "print(\"\\nMerged dataset preview:\")\n",
        "print(merged_df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Handle Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def handle_missing_values(df):\n",
        "    \"\"\"Handle missing values in the merged dataset.\"\"\"\n",
        "    print(\"\\nHandling missing values...\")\n",
        "    print(f\"Missing values before handling:\")\n",
        "    print(df.isnull().sum().sum())\n",
        "    \n",
        "    # Fill numeric columns with 0 or mean\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    for col in numeric_cols:\n",
        "        if col not in [\"City\", \"State\"]:\n",
        "            if \"count\" in col.lower() or \"rate\" in col.lower():\n",
        "                df[col] = df[col].fillna(0)\n",
        "            else:\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "    \n",
        "    # Fill categorical columns\n",
        "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
        "    for col in categorical_cols:\n",
        "        if col not in [\"City\", \"State\"]:\n",
        "            df[col] = df[col].fillna(\"Unknown\")\n",
        "    \n",
        "    print(f\"Missing values after handling: {df.isnull().sum().sum()}\")\n",
        "    return df\n",
        "\n",
        "# Handle missing values\n",
        "merged_df = handle_missing_values(merged_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Processed Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save processed dataset\n",
        "output_path = \"processed_blackspot_dataset.csv\"\n",
        "merged_df.to_csv(output_path, index=False)\n",
        "print(f\"âœ… Saved processed dataset to: {output_path}\")\n",
        "print(f\"Final dataset shape: {merged_df.shape}\")\n",
        "print(f\"\\nColumn names:\")\n",
        "print(merged_df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Dataset Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display summary statistics\n",
        "print(\"Dataset Info:\")\n",
        "print(merged_df.info())\n",
        "print(\"\\n\\nSummary Statistics:\")\n",
        "print(merged_df.describe())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
